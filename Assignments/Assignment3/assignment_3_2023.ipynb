{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](notebook_images/logo.png)\n",
    "\n",
    "# Biometrics System Concepts\n",
    "## Assignment 3: Face Recognition\n",
    "<b>Name</b>: Joe |\n",
    "<b>Student-nr</b>: KD6-3.7 |\n",
    "<b>Date</b>: July 1, 2049\n",
    "---\n",
    "Facial biometrics is among the most common biometric benchmarks. That is due to the easy deployment and implementation of such a system and the lack of requirement of physical interaction by the end-user. The top three application categories where facial recognition are mostly used are: \n",
    "* Security - law enforcement: e.g. to find missing children/disoriented adults, or to identify and track criminals  \n",
    "* Health care: e.g. for detecting genetic diseases, pain management or monitoring response to medication. \n",
    "* Banking and retails: Since the beginning of the digital revolution, facial recognition has been picking up prominence over touch- and type-based interactions because of the convenience it offers without settling on the security of transactions. Facial Recognition softwares for banking usually have a liveness detection which keeps hackers from utilizing an image of the user for impersonation purposes. \n",
    "\n",
    "Any facial recognition system requires the implementation of the following steps:\n",
    "<pre>\n",
    "1. Face detection, to detect faces from a given image.\n",
    "2. Feature extraction, to extract feature vector descriptors from the raw face image.\n",
    "3. Face matching process, for verification or identification purposes.\n",
    "</pre>\n",
    "\n",
    "In this assignment, you will implement, test and report about a face recognition system based on Principal Components Analysis (Eigenfaces), Linear Discriminant Analysis (Fisherfaces), Local Binary Pattern (LBP) and deep learning. The first three procedures are representative of old-school computer vision techniques that are still in use and retain their merits. All four generated feature vector descriptors can be compared for verification/identification.\n",
    "\n",
    "This document is structured as follows:\n",
    "* [Ethics](#Ethics)\n",
    "* [Importing and installing packages](#Importing-and-installing-packages)\n",
    "* [I. Loading data](#I.-Loading-data)\n",
    "    * Read caltech dataset\n",
    "    * Inspect some images   \n",
    "    \n",
    "* [II. Face detection](#II.-Face-detection)\n",
    "    * Detect faces\n",
    "    * Print statistics of data\n",
    "    * Visualise some faces\n",
    "    \n",
    "* [III. Feature-extraction](#III.-Feature-extraction)\n",
    "    * Eigenfaces for face recognition\n",
    "    * Fisherfaces for face recognition\n",
    "    * LBP for face recognition\n",
    "    * Deep metric learning\n",
    "        \n",
    "* [IV. Distance-based and classification-based scoring](#IV.-Distance-based-and-classification-based-scoring)\n",
    "* [V. Evaluation](#V.-Evaluation)\n",
    "    * Validation as verification system\n",
    "    * Validation as identification system\n",
    "    \n",
    "* [VI. Tasks](#VI.-Tasks)\n",
    "    * Mandatory tasks\n",
    "    * Tasks of choice\n",
    "\n",
    "\n",
    "** Note 1: In case you find yourself needing extra computational power you can make use of [google colab](https://colab.research.google.com/). However, implementing overly complicated routines is discouraged. **\n",
    "\n",
    "**Note 2: large parts of this notebook are based on the [PyImageSearch Gurus Course on Computer Vision](https://www.pyimagesearch.com/pyimagesearch-gurus/) by Adrian Rosebrock.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics\n",
    "The ethical and societal challenge posed by data protection is radically affected by the use of facial recognition technologies.\n",
    "\n",
    "In Europe and the UK, the [General Data Protection Regulation](https://www.thalesgroup.com/en/markets/digital-identity-and-security/government/biometrics/biometric-data) (GDPR) provides a rigorous framework for these practices.\n",
    "\n",
    "Any investigations into a citizen's private life or business travel habits are out of the question, and any such invasions of privacy carry severe penalties. \n",
    "\n",
    "Applicable from May 2018, the GDPR supports the principle of a harmonized European framework, in particular protecting the right to be forgotten and the giving of consent through clear affirmative action.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and installing packages\n",
    "Note: this exercise makes use of keras, which is now part of TensorFlow 2.0. It is easy to install using pip, have a look at the [installation guide](https://www.tensorflow.org/install) for more information. Note that you need Python Version 3.5 to 3.7 for this to run (not lower, not higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# the following package contains some handy routines for image manipulation\n",
    "# they have been developed by Adrian Rosenbrock\n",
    "# simply install this package in your environment using \"conda install imutils\"\n",
    "# see https://www.pyimagesearch.com/opencv-tutorials-resources-guides/ for further info\n",
    "import imutils\n",
    "\n",
    "# the following packages are imported when needed throughout this notebook, \n",
    "# we are only listing them here so that you can install all necessary packages upfront.\n",
    "# pandas\n",
    "# sklearn\n",
    "# enum\n",
    "# scipy\n",
    "# tensorflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading data\n",
    "\n",
    "[From OpenCV Docs](https://docs.opencv.org/3.4/da/d60/tutorial_face_main.html):\n",
    "\n",
    "In order to test face recognition systems, we need databases of labeled face images. You can either create your own dataset (but that takes time) or, better, start with one of the available face databases, http://face-rec.org/databases/ gives you an up-to-date overview. Some interesting databases are:\n",
    "\n",
    "* [AT&T Facedatabase](https://cam-orl.co.uk/facedatabase.html). Quoted from http://face-rec.org):\n",
    "> The AT&T Facedatabase, sometimes also referred to as ORL (Olivetti Research Lab) Database of Faces or Olivetti faces, contains ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).The AT&T Facedatabase is good for initial tests, but it's a fairly easy database. The Eigenfaces method already has a 97% recognition rate on it, so you won't see any great improvements with other algorithms.\n",
    "\n",
    "This database can be accessed in a straightforward way through the [sklearn.datasets.fetch_olivetti_faces](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces) module. \n",
    "A preview of this database can be seen [here](https://git-disl.github.io/GTDLBench/datasets/att_face_dataset/) \n",
    "\n",
    "* [The CALTECH Faces dataset](http://www.vision.caltech.edu/html-files/archive.html) is a popular benchmark dataset for face recognition algorithms. Overall, the dataset consists of 450 images of approximately 27 unique people. Each subject was captured under various lighting conditions, background scenes, and facial expressions. Furthermore, bounding box coordinates are provided to crop the faces prior to recognition. A routine (load_caltech_faces)is provided to read the data. \n",
    "\n",
    "![A sample of the CALTECH Faces dataset.](notebook_images/lbps_fr_caltech_faches.jpg)\n",
    "\n",
    "* [The Labeled Faces in the Wild (lfw)](http://vis-www.cs.umass.edu/lfw/). This dataset is a collection of more than 13000 JPEG pictures of famous people collected over the internet. Each face has been labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data set. These images are completely unconstrained (pose, illumination, expression, occlusion).  The only constraint on these faces is that they were detected by the Viola-Jones face detector. Each picture is centered on a single face. A loader [sklearn.datasets.fetch_lfw_people](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html#sklearn.datasets.fetch_lfw_people) is provided by scikit-learn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read caltech dataset\n",
    "For this assignemnt we'll work with the CALTECH dataset because it contains raw images and is not very large. A copy of the dataset is available in the folder \"CalTechFacesDirs\".\n",
    "\n",
    "The data are returned as a list object with the following components:\n",
    "* data: (NxM) matrix of N flattened (linear dimension M) images\n",
    "* images: (Nx(nxm)) matrix of N images of dimension nxm \n",
    "* target: N-dimensional vector of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of each face after preprocessing\n",
    "face_size=(47, 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_img(imagePath):\n",
    "    # load the image and convert it to grayscale\n",
    "    gray = cv2.imread(str(imagePath), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # ROI, and resize it to a canonical size\n",
    "    imagePathStem = str(imagePath.stem)\n",
    "    k = int(imagePathStem[imagePathStem.rfind(\"_\") + 1:][:4]) - 1\n",
    "    \n",
    "\n",
    "    return gray, imagePath.parent.name\n",
    "     \n",
    "# grab in all the subdirs all the image paths associated with the faces\n",
    "datasetPath = Path(\"CalTechFacesDirs\")\n",
    "imagePaths = datasetPath.glob(\"*/*.jpg\")\n",
    "\n",
    "# read image and label information\n",
    "imgs = [read_img(imagePath) for imagePath in imagePaths]\n",
    "data = pd.DataFrame(imgs, columns =['images', 'target']).to_dict('list')\n",
    "\n",
    "# check if all images were found successfully \n",
    "# Please note that 5 images were removed from the original set, because \n",
    "# they wouldn't match the corresponding individual.\n",
    "assert len(data['images']) == len(data['target']) == 445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display data structure\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(5, 21), \n",
    "                 axes_pad=0.1,\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, data['images']):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im, cmap = 'gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Face detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the bounding box coordinates are stored in the <em>CalTechFacesDirs/ImageData.mat</em> file, we'll be computing them from scratch for demonstrative purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Detect faces\n",
    "There are many different face detection algorithms, you can follow the [Learn OpenCV tutorial on Face Detection](https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/) for some (very coarse) background info for two \"conventional\" (HAAR-cascade, HOG) CV methods and two Deep Neural Net (Dlib) based (SSD, MMOD) methods.\n",
    "\n",
    "* Some background on Haar, HOG and MMOD face detectors can be found in this [Guide to Face Detection in Python](https://towardsdatascience.com/a-guide-to-face-detection-in-python-3eab0f6b9fc1). \n",
    "\n",
    "* For MMOD specifically, see: [Max-Margin Object Detection by Davis E. King](http://arxiv.org/abs/1502.00046) and the comments at the beginning of the [dnn_mmod_ex.cpp code](https://github.com/davisking/dlib/blob/master/examples/dnn_mmod_ex.cpp).\n",
    "\n",
    "* For further info on the Single-Shot-Multibox Detector, see [the arXiv publication](https://arxiv.org/abs/1512.02325).\n",
    "\n",
    "In this assignment we'll be using HAAR, you can find more information about this technique on the [OpenCV website](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import Bunch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "feat_data, images, labels = [], [], []\n",
    "\n",
    "\n",
    "for img, label in zip(data['images'], data['target']):\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        img,\n",
    "        scaleFactor = 1.2,\n",
    "        minNeighbors = 9,\n",
    "        minSize = (30, 30),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # iterate faces found in image\n",
    "    for (x, y, w, h) in faces:\n",
    "        new_img = img.copy()\n",
    "        \n",
    "        # extract ROI\n",
    "        raw_face = new_img[y:y+h, x:x+w]\n",
    "\n",
    "        # resize the face to the preferred size\n",
    "        face = cv2.resize(raw_face, face_size)\n",
    "        face_flatten = face.flatten()\n",
    "        \n",
    "        images.append(np.expand_dims(face, axis=-1)), feat_data.append(face_flatten), labels.append(label)\n",
    "\n",
    "# encode classes as integer value\n",
    "label_encoder = LabelEncoder()\n",
    "targets = label_encoder.fit_transform(labels)\n",
    "\n",
    "faces = Bunch(data       = np.array(feat_data), \n",
    "              images     = np.array(images), \n",
    "              labels     = np.array(labels),\n",
    "              target     = np.array(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print statistics of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract number of samples and image dimensions (for later display)\n",
    "n_samples, h, w, n_channels = faces.images.shape\n",
    "imshape = (h, w, n_channels)\n",
    "\n",
    "# count number of individuals\n",
    "n_classes = faces.target.max() +1\n",
    "\n",
    "n_features = faces.data.shape[1]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "print(\"n_features: %d\" % n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualise some faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(10, 10), \n",
    "                 axes_pad=0.1,\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, faces['images']):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(np.mean(im, -1), cmap = 'gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images tend to be large and can differ a lot in lighting, positioning, ... To combat this we'll extract features from each image using 4 different techniques. They all belong to the global/holistic feature representation approaches that map the image to a lower-dimensional subspace through linear and non-linear mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data for training and testing\n",
    "holdout_split = lambda X,y: train_test_split(X, y, test_size=.25, shuffle = True, \n",
    "                                             stratify = y, random_state = 22)\n",
    "\n",
    "class FeatureDescriptor(Enum):\n",
    "    LBP = 1\n",
    "    PCA = 2\n",
    "    LDA = 3\n",
    "    DL = 4\n",
    "\n",
    "######################################\n",
    "# select the technique you want to use\n",
    "DESC = FeatureDescriptor.DL\n",
    "######################################\n",
    "\n",
    "# number of PCA/LDA components to retain\n",
    "num_components = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Eigenfaces for face recognition\n",
    "\n",
    "From [PyImageSearch Gurus Course on Computer Vision](https://www.pyimagesearch.com/pyimagesearch-gurus/) by Adrian Rosebrock:\n",
    ">In 1971 Goldstein et al. [Identification of human faces](https://ieeexplore.ieee.org/document/1450184) demonstrated that facial characteristics can be manually extracted, quantified and used for face recognition. The question then remained: could this be done in an automatic fashion? \n",
    "\n",
    ">It wasn’t until the 1987 paper by Kirby and Sirovich ([A Low-Dimensional Procedure for the Characterization of Human Faces](https://www.researchgate.net/publication/19588504_Low-Dimensional_Procedure_for_the_Characterization_of_Human_Faces)) that we were able to answer this question. This paper is considered to be a seminal work in the history of computer vision — and while other approaches have since been proposed that can outperform Eigenfaces, it’s still important that we take the time to understand and appreciate this algorithm. [Turk and Pentland](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjmvqy6jqfhAhWFZFAKHb74DNgQFjAAegQIABAC&url=http%3A%2F%2Fwww.face-rec.org%2Falgorithms%2Fpca%2Fjcn.pdf&usg=AOvVaw2IlxVps8dldmeA01yQDa_I) built on this idea in the context of face recognition. \n",
    "\n",
    ">The Eigenfaces algorithm uses Principal Component Analysis to construct a low-dimensional representation of face images.\n",
    "\n",
    ">![An example of applying an eigenvalue decomposition to a dataset of faces and extracting the 16 \"eigenfaces\" with the largest corresponding eigenvalue magnitude.](notebook_images/what_is_fr_eigenfaces.jpg)\n",
    "\n",
    "\n",
    ">This involves collecting a dataset of faces with multiple face images per person we want to identify — like having multiple training examples of an image class we would want to label in image classification. Given this dataset of face images, presumed to be the same width, height, and ideally — with their eyes and facial structures aligned at the same (x, y)-coordinates, we apply an eigenvalue decomposition of the dataset, keeping the eigenvectors with the largest corresponding eigenvalues.\n",
    "\n",
    ">Given these eigenvectors, a face can then be represented as a linear combination of what Kirby and Sirovich call eigenfaces.\n",
    "\n",
    ">Face identification can be performed by computing the Euclidean distance between the eigenface representations and treating the face identification as a k-Nearest Neighbor classification problem — however, we tend to commonly apply more advanced machine learning algorithms to the eigenface representations.\n",
    "\n",
    ">Last, it’s important to note that the Eigenfaces algorithm is **not-specific to faces** — we can actually apply it to any arbitrary dataset containing objects of the same type that need to be compared, whether that may be bicycles, cans of soup, or ancient Aztec coins. The Eigenfaces algorithm can be modified to accommodate each of these identification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "if DESC == FeatureDescriptor.PCA:\n",
    "    # Compute a PCA (eigenfaces) on the face dataset\n",
    "    num_components = min(num_components, min(n_samples, n_features))\n",
    "    print(\"num_components {n}\".format(n=num_components))\n",
    "    desc = PCA(n_components=num_components, svd_solver='randomized', whiten=True).fit(faces.data)\n",
    "    X_pca = desc.transform(faces.data)\n",
    "    embedded = X_pca\n",
    "    \n",
    "    dist_metric = euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fisherfaces for face recognition\n",
    "\n",
    "From [Scholarpaedia](http://www.scholarpedia.org/article/Fisherfaces):\n",
    "> When the goal is classification rather than representation, PCA may not yield the most desirable results. In such cases, one wishes to find a subspace that maps the sample vectors of the same class in a single spot of the feature representation and those of different classes as far apart from each other as possible. The techniques derived to achieve this goal are known as discriminant analysis (DA).\n",
    "The most known DA is Linear Discriminant Analysis (LDA), which can be derived from an idea suggested by R.A. Fisher in 1936. When LDA is used to find the subspace representation of a set of face images, the resulting basis vectors defining that space are known as [Fisherfaces](http://www.scholarpedia.org/article/Fisherfaces).\n",
    "\n",
    "![Example of FisherFaces](notebook_images/FisherFaces.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "if DESC == FeatureDescriptor.LDA:\n",
    "    num_components = min(num_components, min(n_classes - 1, n_features))\n",
    "    desc = LinearDiscriminantAnalysis(n_components=num_components).fit(faces.data, faces.target)\n",
    "    X_lda = desc.fit_transform(faces.data, faces.target)\n",
    "    embedded = X_lda\n",
    "    \n",
    "    dist_metric = euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LBP for face recognition\n",
    "\n",
    "Local Binary Patterns, or LBPs for short, are a texture descriptor made popular by the work of Ojala et al. in their 2002 paper, [Multiresolution Grayscale and Rotation Invariant Texture Classification with Local Binary Patterns](https://ieeexplore.ieee.org/document/1017623) (although the concept of LBPs were introduced as early as 1993).\n",
    "\n",
    "Unlike [Haralick texture features](http://haralick.org/journals/TexturalFeatures.pdf) that compute a global representation of texture based on the [Gray Level Co-occurrence Matrix](https://en.wikipedia.org/wiki/Co-occurrence_matrix), LBPs instead compute a local representation of texture. This local representation is constructed by comparing each pixel with its surrounding neighborhood of pixels. See [PyImageSearch](https://www.pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-opencv/) and [Scikit-Image](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_local_binary_pattern.html) for further details on LBP.\n",
    "![LBP](notebook_images/LBP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from localmodules.local_binary_patterns import LBP\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "if DESC == FeatureDescriptor.LBP:\n",
    "    desc = LBP(numPoints=8, radius=1, grid_x=7, grid_y=7)\n",
    "    embedded = desc.describe_list(faces.images[...,0])\n",
    "    \n",
    "    dist_metric = chisquare\n",
    "    \n",
    "# Hint: If you get NaN values as a result of Chi-Square, feel free to use the following function:\n",
    "\n",
    "def CHI2(histA, histB, eps=1e-10):\n",
    "    # compute the chi-squared distance\n",
    "    d = 0.5 * np.sum(((histA - histB) ** 2) / (histA + histB + eps))\n",
    "\n",
    "    # return the chi-squared distance\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep metric learning\n",
    "Similar to previous techniques, we can use deep learning to generate an enriched representation of faces that you can use in simple or more complicated classification algorithms. \n",
    "Instead of building networks that are trained end-to-end to perform face recognition, we will use deep metric learning. Metric learning based methods learn a projection of the input data on a low-dimensional vector, such that classification in that lower-dimensional space is much easier and can be performed using simple distance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of face recognition we can observe the following major Deep Learning based approaches since 2014.\n",
    "\n",
    "1. The first approach was developed and presented in 2014 by researchers at Facebook and is called [DeepFace](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf) . The major contributions where a CDNN to classify faces. In addition a 3D/2D alignment procedure is applied prior to classification. \n",
    "The CDNN is trained as a classifier on the SFC (Social Face Classification) dataset, which is a very large (4.4 million image of 4000 individuals) dataset. This setting is called supervised.  The challenge is whether this can also be generalizd to other datasets and subjects as well. By training it on the large collection of uncontrolled images, an embedded representation (the vector description prior to the classification layer) can be generated that is, allegedly, generalizable to other face recognition data. One can then use this representation as we have previously done for classical representations. \n",
    "By way of demonstration they tested it on the LFW dataset to achieve >0.95% verification accuracy. <br> \n",
    "\n",
    "2. The second approach was developed by researchers at Google.  An in-depth presentation of their FaceNet/OpenFace deep CNN's for metric embedding of faces is given by [Martin Krasser et al](http://krasserm.github.io/2018/02/07/deep-face-recognition/). Note that this method was trained on 200 million images of eight million identities. The main methodological point is that these networks are trained using a special loss-function: the triplet loss function. \n",
    ">This function is minimized when the distance between an anchor image $x_{ai}$ and a positive image $x_{pi}$ (same identity) in embedding space is smaller than the distance between that anchor image and a negative image $x_{ni}$ (different identity) by at least a margin $\\alpha$. Model training aims to learn an embedding f(x) of image x such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There is a very elegant Python implementations available of the FaceNet/OpenFace approach. It is nicely described in [this PyImageSearch tutorial](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/). It explicitly includes the Dlib package with the [face_recognition modules](https://face-recognition.readthedocs.io/en/latest/index.html) for face detection and recognition. Especially the latter package provides a very convenient interface. \n",
    "\n",
    "From the [Dlib face_recognition.py](https://github.com/davisking/dlib/blob/master/python_examples/face_recognition.py) code, we get an application view of the embedding:\n",
    "\n",
    "> This example shows how to use dlib's face recognition tool.  This tool maps an image of a human face to a 128 dimensional vector space where images of the same person are near to each other and images from different people are far apart.  Therefore, you can perform face recognition by mapping faces to the 128D space and then checking if their Euclidean distance is small enough. \n",
    "\n",
    "> When using a distance threshold of 0.6, the dlib model obtains an accuracy of 99.38% on the standard LFW face recognition benchmark, which is comparable to other state-of-the-art methods for face recognition as of February 2017. This accuracy means that, when presented with a pair of face images, the tool will correctly identify if the pair belongs to the same person or is from different people 99.38% of the time.\n",
    "\n",
    "In the [DeepFace](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf) paper they also tested a [siamese network](https://en.wikipedia.org/wiki/Siamese_network) DNN architecture for generating embedded vector representations.  This consists of two copies of the same CNN (sharing their weights) that are applied to pairs of images. During training the distance between the embedded representations of the same individual is minimized, and the distances between embedded representations of different individuals is maximized. In the mean time, this work was extended, steadily increasing the performance. Siamese networks are an example of *One-shot Learning* where you tyically have very few samples per object or dynamically changing number of objects/subjects. While [this link](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d) tests it on a non-face recognition task, it can be applied to face recognition as well.\n",
    "\n",
    "The localmodules.siamese file contains the description of a shallow CNN model that is trained with [contrastive loss](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).\n",
    "This subnetwork is copied twice and the output of both siamese copies are then passed onto a vector euclidean distance (ED) calculation layer. The network gets as input pairs of imposter or genuine images with the associated label (genuine = 0, imposter = 1). You can make use of the output of the network or the output before the ED layer (embedded) in your validation.\n",
    "\n",
    "Note: this part on Siamese networks is adapted from [Packt tutorial](https://hub.packtpub.com/face-recognition-using-siamese-networks-tutorial/), which seems to be a re-implementation from  [this Packt book on Neural Network Projects with Python](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789138900)). See also [this Hackernoon tutorial](https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e).\n",
    "\n",
    "Note: you can change the CNN at your will, but beware that it might take additional training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter problem importing the siamese model, consider using Google Colab.\n",
    "import localmodules.siamese as siamese\n",
    "\n",
    "\n",
    "encoder, model = siamese.create_siamese_model(imshape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize your model structure:\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the complexity of the Siamese network this might take a while. You can also change the number of epochs to improve the training (with the risk of overfitting). You will notice that a small number (<10) of epochs already gets you a decent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import localmodules.siamese as siamese\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if DESC == FeatureDescriptor.DL:\n",
    "    x_train, x_test, y_train, y_test = holdout_split(*siamese.get_siamese_paired_data(faces.images, faces.target))\n",
    "    rms = Adam()\n",
    "    model.compile(\n",
    "        loss=siamese.contrastive_loss, \n",
    "        optimizer=rms, \n",
    "        metrics = [siamese.accuracy],\n",
    "        run_eagerly=True)\n",
    "\n",
    "    epochs = 10\n",
    "    model.fit([x_train[:, 0], x_train[:, 1]], y_train, \n",
    "              validation_split=0.2,\n",
    "              batch_size=32, verbose=2, epochs=epochs)\n",
    "    \n",
    "    test_scores = model.predict([x_test[:, 0], x_test[:, 1]])\n",
    "    test_acc = accuracy_score(y_test, test_scores > 0.5)\n",
    "    print(\"Accuracy on the test set: {}\".format(test_acc))\n",
    "    embedded = encoder(faces.images.astype(np.float32)).numpy()\n",
    "\n",
    "    dist_metric = euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Distance-based and classification-based scoring\n",
    "\n",
    "Biometrics is based on generating pairwise matching scores. In it's simplest form, the score is computed based on the distance metrics (distance-based scoring). E.g. a simple pairwise comparision (L2-distance, e.g.) can be used. \n",
    "\n",
    "* In verification mode one compares this matching score to a decision threshold in a 1-to-1 setting.\n",
    "* In identifcation mode one uses this matching score to rank the templates in the database and, possibly, thresholding it as well in a 1-to-N setting\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, some classification algorithms provide us with the classification scores (or probabilities), which are related to the likelihood that an image belongs to each subject in the dataset. These classification scores can be seen as an advanced classification-based matching score. \n",
    "\n",
    "* In verification mode one compares the soft scores (or probabilities) of the classification with a threshold to verify whether the person is who he claims to be.\n",
    "* In identifcation mode one sorts the soft scores to propose the most similar people in the dataset.\n",
    "\n",
    "To implement a classification-based system (optional task 3), the following steps are expected:\n",
    "1. For each person in the dataset leave one image out (E.g. the first image in the dataset) for the testset. This means the size of the test set equals the number of individuals in the data (=n).\n",
    "2. Run the classifier of your choice and generate the classification probabilities for each test image.\n",
    "3. Build your n by n similarity matrix, in which each row corresponds to a test image and each column corresponds to an individual in the dataset.\n",
    "4. Calculate the evaluation metrics (E.g. ROC) based on the obtained similarity matrix to test your system in a verification or identification scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q1: </b> Compute distance-based pair-wise matching scores.\n",
    "</div>\n",
    "\n",
    "Given the function <em>dist_metric</em> and the vector representations in variable <em>embedded</em>, compute the pairwise distances. Do this for PCA, LDA, LBP and a DL facial representations, given the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Evaluation\n",
    "\n",
    "In this section you can re-use the code you developed for validation in the previous assignments, and in particular assignment 1. \n",
    "\n",
    "### 1. Validation as verification system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q2: </b> Compute F1 and accuracy scores for variable (and optimal) thresholds\n",
    "</div>\n",
    "\n",
    "* Calculate and plot F1 and accuracy scores for a range (min, max) of thresholds.\n",
    "* Determine some optimal threshold (look up in classification literature). Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q3: </b> Plot genuine and impostor scores\n",
    "</div>\n",
    "When comparing the different feature extractions/facial representations, discuss the difference in the overlap between genuine and imposter scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q4: </b> Perform a full-on verification assessment based on the scores obtained. Interpret the results.\n",
    "</div>\n",
    "\n",
    "Hint: Calculate and plot the equal error rate. Plot the precision-recall curve and compute AUC and average precision. Interpret the results e.g. by explaining what aspect of the system performance is explored by each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation as identification system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q5: </b> Validate the systems in an identification scenario. \n",
    "</div>\n",
    "\n",
    "Hint: Generate a Cumulative Matching Curve. Calculate and compare rank-1 performance among the four feature extraction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Tasks\n",
    "All tasks should be accompanied by a text providing information on the technique, steps, and the reasoning behind choosing the techniques.\n",
    "\n",
    "\n",
    "### 1. Mandatory tasks\n",
    "<b>The student should do all of the tasks indicated below</b>: \n",
    "1. Execute all the tasks indicated in the document above\n",
    "2. Compare the 4 feature representations (LBP, PCA, LDA, DL using Siamese Networks) using the implemented validation metrics in Questions 2-5, and report on the performance. Please make it clear where in your report the comparison is made for each question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tasks of choice\n",
    "<b>The student should choose a number of tasks equivalent to <u>at least 3pts</u> from the list below (pts are not related to the grades)</b>. \n",
    "\n",
    "1. Evaluate your system on the other two datasets (AT&T, LFW). Feel free to subsample datasets if they are too memory-consuming on your system. (1pt.)\n",
    "2. Implement 2 different face detectors and compare all techniques to the ground truth bounding boxes provided in <em>CalTechFacesDirs/ImageData.mat</em>. Look up the literature for methods to compare different face detectors. (1pt.)\n",
    "3. Implement a classification-based scoring method, using an advanced classifier of your choice. Evaluate this system in an identification and verification scenario. (Hint: Follow steps introduced in section IV. Distance-based and classification-based scoring) (2pt.)\n",
    "4. Experiment with the Siamese deep learning model by implementing a different loss function or a different distance calculation layer. (1pt.)\n",
    "5. Pre-train your deep learning network on a different (larger) dataset and then fine-tune it with CALTECH and compare the results. (2pt.) \n",
    "6. Implement a different deep learning model* (2pt.)\n",
    "\n",
    "\n",
    "e.g. performing task 4 and 5 is valid because their points sum up to 3. \n",
    "\n",
    "<em>Note: Indicate clearly which tasks you end up choosing and where we can find the implementations and/or results. </em>\n",
    "\n",
    "[*] The implemented network has to be significantly different, adding some layers like batch normalisation, dropout, ... to the current network or increasing the kernel size, amount of filters and activation functions does not count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
