{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logo.png)\n",
    "# Biometrics System Concepts\n",
    "## Assignment 1: Evaluating performance of biometric systems\n",
    "<b>Name</b>: Marty McFly |\n",
    "<b>Student-nr</b>: C25 |\n",
    "<b>Date</b>: June 12, 2015\n",
    "---\n",
    "\n",
    "In this assignment we focus on evaluating the performance of any biometric system in a verification and identification setting. \n",
    "\n",
    "Before you get started you should be able to properly distinguish verification from identification and know the corresponding validation procedures. If this is not the case for you yet, **review the lecture notes!** We will give a short recap to refresh your memory: \n",
    "* **VERIFICATION** (a.k.a. authentication): Authenticating a claimed identity (is this person who he/she claims to be?).\n",
    "\n",
    "* **IDENTIFICATION**: Associate a particular individual with an identity (who is this unidentified individual?).\n",
    "\n",
    "This document is structured as follows:\n",
    "\n",
    "- [I. Reading the data](#I.-Reading-the-data)\n",
    "- [II. Validation of verification system](#II.-Validation-of-verification-system)\n",
    "- [III. Validation of identification system](#III.-Validation-of-identification-system)\n",
    "- [IV. Assignment Instructions](#IV.-Assignment-Instructions)\n",
    "\n",
    "\n",
    "Code examples will be provided below. You can and are invited to adapt these at your will (different parameter settings, different choices of alogorithmic components). The code examples in this assignment are just sekeleton code,  **adapt where needed! And try to keep things structured!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install and import the required python packages to run this notebook. Feel free to add more packages whenever needed.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# the following meta-command is required to get plots displayed in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# package for reading xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "\n",
    "# package for data analysis with fast and flexible data structures\n",
    "import pandas as pd\n",
    "\n",
    "# package to show a nice graphical progress-bar for lengthy calculations\n",
    "# docu and installation on https://tqdm.github.io\n",
    "# if you have difficulties installing this package: \n",
    "# - make sure your jupyter lab is up to date\n",
    "# - https://github.com/tqdm/tqdm/issues/394#issuecomment-384743637\n",
    "# - consider just leaving it out (just remove the 'tqdm_notebook' in the code)\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "# import local modules for reading and converting BSSR1 fingerprint score data\n",
    "import src.BSSR1 as BSSR1\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "Many operations have already been implemented in <a href=\"https://docs.scipy.org/doc/numpy/index.html\">SciPy</a>, feel free to use them or any other unless explicitely stated not to in the assignment.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Reading the data\n",
    "In this assignment we focus on the validation of a pre-exising biometric system. We will make use of actual predicted fingerprint similarity scores, from the left and right index fingers. This bypasses all steps of preprocessing, feature extraction and matching and allows us to concentrate on the score evaluation procedures. The scores are the result of comparing an enrolled user's image with that of the same (genuine scores) or another user (impostor scores). The data (biometrics scores set BSSR1) are made available through the American National Institute of Standards and Technologies [(NIST)](https://www.nist.gov/itl/iad/image-group/nist-biometric-scores-set-bssr1). They are provided as part of your assignment folder in the `data/fing-x-fing` subfolder. \n",
    "\n",
    "In this code example we will compare the performance of a biometric system that was based on the right index finger to a biometric system based on the left index finger. These systems are referred to as `ri` and `li` respectively in both code and text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the similarity matrix and reading the genuine and impostor scores\n",
    "We provide you with the code to read the scores from the files (normally you can leave this code as-is).\n",
    "\n",
    "The code consits of 2 steps that are executed for both the `ri` and `li` system: \n",
    "1. Scores are converted to similarity matrices. To simplify the task we have provided you with the similarity matrices. You need to load the similarity matrices with the genuine scores on the diagonal and the impostor scores on the off-diagonal elements.  \n",
    "2. Convert the original scores to a linear list of scores with associated labels of genuine or impostor (simplifies use of [scikit-learn](https://scikit-learn.org/stable/index.html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1: loading the similarity matrices for left and right index fingers\n",
    "with open('./data/li_similarity_matrix.pickle', 'rb') as f:\n",
    "    li_similarity_matrix = pickle.load(f)\n",
    "with open('./data/ri_similarity_matrix.pickle', 'rb') as f:\n",
    "    ri_similarity_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: convert to genuine and impostor scores, the *_genuine_id provides a mask for the genuine scores\n",
    "li_genuine_id, li_scores = BSSR1.sim2scores(li_similarity_matrix)\n",
    "ri_genuine_id, ri_scores = BSSR1.sim2scores(ri_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "In this assignment we focus only on evaluating biometric systems. It is in your best interest to write your code such that it can easily be reused in the upcoming assignments, where you will have to develop entire biometric system pipelines.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Validation of verification system\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "\n",
    "The performance of a verification scenario can be expressed in a number of ways (see [Jain et al.][1] section 1.4.1.1 for more information). In essence one has a binary classification problem: is it the claimed identity or not? \n",
    "\n",
    "We denote our classes as:\n",
    "<ol start=\"0\">\n",
    "  <li>Impostor (False),</li>\n",
    "  <li>Genuine (True).</li>\n",
    "</ol>\n",
    "\n",
    "Furthermore, we represent the set of scores as s, the imposter event as $I$ and a genuine event as $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation using FMR, FRR, ROC and Precision/Recall curves\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "\n",
    "\n",
    "\n",
    "#### 1.1 Genuine and impostor score distributions\n",
    "Given the genuine and impostor scores (from section I), we can plot the imposter $p(s | I)$ and genuine $p(s | G)$ distribution to gain some first insights in the system. The result should look something like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/ScoreDistributions.png\" width=\"250\" height=\"auto\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q1: </b> Score distributions\n",
    "<ul>\n",
    "  <li>Plot the genuine and impostor score distributions in a single plot.</li>\n",
    "  <li>Do you need to normalize the distributions? Why (not)?</li>\n",
    "  <li>Describe qualitatively this combined plot (hint: limit the score range for better understanding)</li>\n",
    "</ul>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the genuine and imposter score distributions.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. FMR, FRR and Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "The 'false accept' and 'false reject' regions in the illustration above are quantified using the False Match/Acceptance Rates (FMR/FAR) and False Non-Match/Rejections Rates (FNMR/FRR). The FMR and FNMR can easily be computed given the previously calculated probability distributions and a threshold value $\\eta$. Practically this boils down to a counting problem, having $\\mathcal{I}$ the indicator function (return 1 if x is true, else 0) we can compute:\n",
    "$$\n",
    "FMR(\\eta) = p(s \\geq \\eta | I) \\approx \\frac{1}{|I|} \\sum_{s \\in I} \\mathcal{I}(s \\geq \\eta),\n",
    "$$\n",
    "$$\n",
    "FRR(\\eta) = p(s < \\eta | G) \\approx \\frac{1}{|G|} \\sum_{s \\in G} \\mathcal{I}(s < \\eta).\n",
    "$$\n",
    "\n",
    "We can also compute the Genuine Acceptance Rate (GAR)/True Match Rate (TMR) as:\n",
    "$$\n",
    "GAR(\\eta) = p(s \\geq \\eta | G) = 1 - FRR(\\eta).\n",
    "$$\n",
    "\n",
    "Note that choosing a threshold is always a tradeof between FMR and FNMR. <br>\n",
    "<img src=\"img/FAR_FRR.png\" width=\"300\" height=\"auto\" align=\"center\"/>\n",
    "\n",
    "To observe the impact of the threshold value, one often plots a [Receiver Operating Characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. Usually in these curves the GAR/TMR is plotted against the FMR for different decision threshold values $\\eta$. For those familiar with binary classification systems outside of the biometrics literature; the GAR/TMR is often referred to as the True Positive Rate (TPR), sensitivity or recall and FMR is also known as the False Positive Rate (FPR) or the False Accept Rate (FAR). \n",
    "\n",
    "<img src=\"img/ROC.png\" width=\"700\" height=\"auto\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q2: </b> ROC Curves\n",
    "<ul>\n",
    "    <li>Calculate FPR, TPR from the matching scores.</li>\n",
    "    <li>Plot FAR and FRR as a function of matching scores.</li>\n",
    "    <li>Plot the ROC curve. Plot for linear and logarithmic scale if needed. What do you observe?</li>\n",
    "    <li>Plot the Detection Error Trade-off (DET) curve. How does it compare to ROC?</li>\n",
    "</ul>  \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "We highly recommend you use the <a href=\"https://scikit-learn.org/stable/index.html\">scikit-learn</a> package, it contains standard and advanced routines for machine learning, including classificaton and validation algorithms.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate FPR, TPR from the matching scores.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot FAR and FRR as a function of the decision threshold.\"\"\"\n",
    "# FAR <=> FPR\n",
    "# FRR <=> FNR <=> FN/P <=> 1-TPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"plot the ROC curve (TPR against the FPR for different threshold values)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"plot the DET curve (FRR (=1-tpr) against the FAR for different threshold values)\"\"\"\n",
    "\n",
    "# FAR <=> FPR\n",
    "# FRR <=> FNR <=> FN/P <=> 1-TPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. F1 and accuracy as metrics\n",
    "\n",
    "While biometric systems are, traditionally, evaluated using FMR and FRR and ROC/DET curves, we can also have a look at traditional [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) such as classification accuracy (or error) and F1 measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q3: </b> Classification Metrics\n",
    "<ul>\n",
    "    <li>Plot F1 and accuracy as a function of the decision thresholds on the similarity score.</li>\n",
    "    <li>Calculate the threshold and accuracy for which F1 is maximal. Is it an interesting operating point?</li>\n",
    "    <li>Do the same for the classification error (accuracy). Is there a difference?</li>\n",
    "    <li>Is accuracy a good performance metric in this case?</li>\n",
    "</ul>  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "We highly recommend you use the <a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\">scikit-learn classification metrics</a> to assist.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot F1 and accuracy as a function of the decision thresholds on the similarity score.\"\"\"\n",
    "# Hint: evaluating for ± 50 threshold values should suffice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the threshold for which F1 is maximal.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Do the same for the classification error (or accuracy)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. AUC and EER as summary measures\n",
    "\n",
    "The overall performance (over all threshold settings) is typically expressed through:\n",
    "\n",
    "* The Area Under the Curve (AUC) (with TPR((y-axis) vs FPR (x-axis))\n",
    "\n",
    "<img src=\"img/AUC.png\" width=\"250\" height=\"auto\"/>\n",
    "\n",
    "The AUC can be used to compare different systems. The larger this number, the better.\n",
    "However, since it is a summary measure, always inspect the full ROC curve to make decisions about performance given operating conditions (in wich FRR, FAR regime to work e.g.).\n",
    "\n",
    "* The Equal Error Rate (EER), which is the point on the ROC-curve where FAR(FMR) equals FRR (1-TAR). A lower EER value indicates better performance. \n",
    "\n",
    "<img src=\"img/EER.png\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q4: </b> AUC, EER and alternatives\n",
    "<ul>\n",
    "    <li>Calculate ROC AUC. Is this a good metric? What does it reveal about the system? </li>\n",
    "    <li>Calculate (by approximation) the EER and plot it on the FAR-FRR curve. Is this a good peration point?</li>\n",
    "    <li>Calculate the decision threshold for which the sum of FRR and FAR is minimal. Is this point similar to the total classification error?</li> \n",
    "    <li>Can you suggest other strategies that give you an \"optimal\" performance? Calculate and discuss their (de)merits.</li>\n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the ROC AUC.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate (by approximation) the EER and plot it on the FAR-FRR curve.\"\"\"\n",
    "# hints:\n",
    "#  - avoid using a library that directly computes the EER for this assignment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the decision threshold for which the sum of FRR and FAR is minimal.\"\"\"\n",
    "# hint: same as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluation using Precision and Recall\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "[3]: <https://en.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision> (Average precision)\n",
    "\n",
    "In a general binary classification setting, one also often presents Precision-Recall curves. PR-curves are sometimes summarized using the [average precision scores][3]. How and when to use ROC or PRC is discussed [here][2]. A more general discussion of these measures is provided [here](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c).  \n",
    "\n",
    "Scikit-Learn provides routines for calculating these curves and numbers as demonstrated in the code below from this [link][2], it also provides an implementation of the [average precision scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q5: </b> Precision-Recall curves and related summary measures\n",
    "<ul>\n",
    "    <li>Calculate and plot the Precision-Recall curve for this system. What does it reveal about the performance of the system?</li>\n",
    "    <li>Calculate the Area Under the PR-curve. Discuss.</li>\n",
    "    <li>Calculate the average precision scores. Discuss its value.</li> \n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate and plot the Precision-Recall curve for this system\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the Area Under the PR-curve.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the average precision scores\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Validation of identification system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using CMC curves\n",
    "In an identification scenario one has a 1-to-many or multi-class classification problem. The performance of such a system is typically measured by the Cumulative Match Characteristic (CMC) curve. This curve plots the experimental probability that a correct identification is returned within the top-x (x=1, ..., N) ranked matching scores. \n",
    "\n",
    "<img src=\"img/CMC.jpg\" width=\"500\" height=\"auto\"/>\n",
    "\n",
    "[Bolle et al.](https://ieeexplore.ieee.org/document/1544394) show that:\n",
    "> the CMC is also related to the FAR and FRR of a 1:1 matcher, i.e., the matcher that is used to rank the candidates by sorting the scores. This has as a consequence that when a 1:1 matcher is used for identification, that is, for sorting match scores from high to low, the CMC does not offer any additional information beyond the FAR and FRR curves. The CMC is just another way of displaying the data and can be computed from the FAR and FRR.\n",
    "\n",
    "This paper is not mandatory but those interested can have a look at it.\n",
    "\n",
    "CMC curves can easily be generated once you have the ranked matching scores for every test sample. In our example we can easily calculate it from the similarity matrix (note that in this very particular case we have only one genuine pair per test sample). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q6: </b> CMC curves\n",
    "<ul>\n",
    "    <li>Calculate the Cumulative Matching Characteristic curve (implement this yourself)</li>\n",
    "    <li>Compute the Rank-1 Recognition Rate.</li>\n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the Cumulative Matching Characteristic curve.\"\"\"\n",
    "# Hint: don't use a library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"compute the Rank-1 Recognition Rate.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q7: </b> Evaluate different biometric systems\n",
    "<ul>\n",
    "    <li>Use above evaluation techniques to compare the biometric system based on the left index to the right index </li>\n",
    "    <li>Do you see any differences in any of the curves or measures?</li>\n",
    "</ul>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Assignment Instructions\n",
    "For this assignment you have to submit a report (.pdf) and the implementation of this notebook (.ipynb) to toledo. The report should be between 3-8 pages (more pages $\\nRightarrow$ higher score) and should be structured around the posed questions (Q1,...). The text should demonstrate your understanding of the material and, depending on the question, clearly introduce the context, technique, your expectation and interpretation of the results. Do not limit yourselves to just answering the questions. Feel free to add figures and, if needed, some *small* code snippet to clarify your position. **The report should be self contained, the notebook functions as supplementary material only!**\n",
    "\n",
    "*Note: Make sure you include all the files required to run the notebooks on submission.* <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f2150e378602eeee6d0304e9ccaa26dd052c276dae5ddc753a78b9079576b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
